#### 决策树的构造

优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。

缺点：可能会产生过度匹配问题。

适用数据类型：数值型和标称型

在构造决策树时，注意的是**当前数据集上哪个特征在划分数据分类时起决定性作用**。为了找到局定性的特征、划分出最好的结果，我们必须评估每个特征，完成测试之后，原始数据集就被划分为几个数据子集。这些数据子集会分布在第一个决策点的所有分支上。如果某个分支下的数据属于同一类型，则当前无需阅读的垃圾邮件已经正确地划分数据分类。

伪代码

```
检测数据集中的每个子项是否属于同一分类：
	If so return 类标签；
	Else
		寻找划分数据集的最好特征
		划分数据集
		创建分支节点
			for 每个划分的子集
				调用函数createBranch并增加返回结果到分支节点
		return 分支节点
```

决策树的一般流程

- 收集数据：可以使用任何方法。
- 准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化。
- 分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。
- 训练算法：构造树的数据结构。
- 测试算法：使用经验树计算错误率。
- 使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义。

熵定义为信息的期望值，在此之前，需要知道信息的定义。如果待分类的事务可能划分在多个分类之中，则符号$x_i$ 的信息定义为

$l(x_i)=-log_2{p(x_i)}$

其中$p(x_i)$是选择该分类的概率。

为了计算熵，我们需要计算所有类别所有可能值包含的信息期望值，通过下面公式得到：

$H = -\sum{n\atop i-1}p(x_i)log_2{p(x_i)}$

