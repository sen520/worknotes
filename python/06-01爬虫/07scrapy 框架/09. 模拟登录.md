##### 模拟登录

> 访问登录页面后再登录

- 首先会访问 `start_urls` 里面的 URL 
- 然后响应会传递给 parse 函数，然后就由这个进行登录
- 利用 `scrapy.FormRequest` 进行登录 (这是 POST 请求)
- 登录成功后，他会自动携带 Cookie 到以后每一个请求 （保持登录状态）

```python
# -*- coding: utf-8 -*-
import scrapy

class WebSpider(scrapy.Spider):
    name = 'web'
    allowed_domains = ['www.sxt.cn']
    start_urls = ['http://www.sxt.cn/index/login/login.html']
	
    # 这里是发送了一个 POST 请求，同时把用户名和密码传给了服务器
    def parse(self, response):
        return scrapy.FormRequest(url="http://www.sxt.cn/index/login/login.html",
                                  formdata={'user': '15021688057', 'password': '123456'},
                                  callback=self.after_login)

    def after_login(self, response):
        print(response.text)
```

> 直接把用户名密码传给登录页面 

- 使用 `start_request ()`进行登录 (==注意 start_request 要使用 yield== ) 
- 后续的操作会自动携带 cookie

```python
# -*- coding: utf-8 -*-
import scrapy
class WebSpider(scrapy.Spider):
    name = 'web'
    allowed_domains = ['www.sxt.cn']

    # start_urls = ['http://www.sxt.cn/index/login/login.html']

    
    # start_requests() 默认情况下是读取 start_urls 里面的，既然我们重写了 start_requests
    # 那么就可以不用些 start_urls 了
    
    def start_requests(self):
        yield scrapy.FormRequest(url="http://www.sxt.cn/index/login/login.html",
                                 formdata={'user': '15021688057', 'password': '123456'},
                                 callback=self.parse)

    def parse(self, response):
        print(response.text)

```



##### 直接使用cookie登录

第一个请求设置了 cookie 后，后面的请求，都会自动携带 cookie

```python
# -*- coding: utf-8 -*-
import scrapy


class WebSpider(scrapy.Spider):
    name = 'web'
    allowed_domains = ['www.sxt.cn']

    def start_requests(self):
        
        # 这里是之前自己通过浏览器登录后获取的 cookie
        cookies = {
            'session': 'eyJfcGVybWFuZW50Ijp0cnVlLCJ1c2VyX2lkIjoxfQ.Dh3xlQ.ACcBOBlWJ5CckArtZokh9MLkYj0'
        }
        
        # 我们在访问页面的时候，设置 cookies 参数
        yield scrapy.Request(url='http://74.120.174.165:8888/add_content/', callback=self.parse, cookies=cookies)

    def parse(self, response):
        print(response.text)

```



##### 使用 `cookiejar` 登录

==使用 cookiejar 的好处是，每一次登录 我们可以使用不同的 cookie，虽然自己也可以实现，但是框架已经帮我们实现了。==

==同时 cookiejar 也接受服务器修改我们的 cookie==

每一次使用 Request 都需要我们再次设置 `cookiejar`

```python
# -*- coding: utf-8 -*-
import scrapy


class WebSpider(scrapy.Spider):
    name = 'web'
    allowed_domains = ['74.120.174.165']

    def start_requests(self):
        form_data = {
            'email': '1120159006@qq.com',
            'password': 'leixianyang1996'
        }
        # 首次登录用 meta = {'cookiejar':1}  开启 cookiejar
        # 使用了 cookiejar 后，cookie 是不会自动携带的，需要手动的设置 meta={'cookiejar': response.meta['cookiejar']}
        yield scrapy.FormRequest(url='http://74.120.174.165:8888/login/', meta={'cookiejar': 1}, formdata=form_data)

        # 后面使用的话，就要用 meta={'cookiejar': response.meta['cookiejar']}
        # 这样之前的 cookie 就会自动携带
    def parse(self, response):
        yield scrapy.Request(url='http://74.120.174.165:8888/add_content/', callback=self.test,
                             meta={'cookiejar': response.meta['cookiejar']})

    def test(self, response):
        print(response.text)

```

#### 模拟登录例子