`spider.py`

```python
import threading
import requests
import re
from lxml import etree
from config import url_queue, next_queue, content_queue, GetThreadNum, RequestThreadNum, ParseThreadNum
import queue
import time

flag1 = []
flag2 = []


class ThreadGetURL(threading.Thread):
    def __init__(self, next_queue, url_queue):
        super().__init__()
        self.next_queue = next_queue
        self.url_queue = url_queue

    def run(self):
        while self.next_queue.scard('nextURLs'):
            re = requests.get(self.next_queue.spop('nextURLs'))
            re.encoding = 'gb2312'
            current_page = re.text
            self.get_url(current_page)
            print('GetURLs ' + threading.current_thread().getName() + ' 正在收集 URL')
        flag1.append('a')

    def get_url(self, current_page):

        html = etree.HTML(current_page)

        URLs = html.xpath('//div[@class="co_content8"]/ul/td/table/tr[2]/td[2]/b/a[last()]/@href')
        for item in map(lambda item: 'https://www.dy2018.com' + item, URLs):
            self.url_queue.sadd('URLs', item)


class ThreadRequest(threading.Thread):

    def __init__(self, url_queue, content_queue):
        super().__init__()
        self.url_queue = url_queue
        self.content_queue = content_queue

    def run(self):
        while len(flag1) != GetThreadNum or self.url_queue.scard('URLs') != 0:
            current_url = self.url_queue.spop('URLs')
            if current_url:
                re = requests.get(current_url)
                re.encoding = 'gb2312'
                html = re.text
                self.content_queue.sadd('content', html)
                print('Request ' + threading.current_thread().getName() + ' 正在收集数据')
        flag2.append('a')


class ThreadParse(threading.Thread):

    def __init__(self, content_queue):
        super().__init__()
        self.content_queue = content_queue

    def run(self):
        while len(flag2) != RequestThreadNum or self.content_queue.scard('content') != 0:
            current_content = self.content_queue.spop('content')
            if current_content:
                self.parse(current_content)

    def parse(self, content):
        time.sleep(0.2)
        print('Parse ' + threading.current_thread().getName() + ' 正在-------解析--------数据')
        pass


if __name__ == '__main__':

    def get_next(start_url):
        re = requests.get(start_url)
        re.encoding = 'gb2312'
        html = etree.HTML(re.text)
        nextURL = html.xpath('//div[@class="x"]/p/select/option/@value')
        for item in map(lambda item: 'https://www.dy2018.com' + item, nextURL):
            next_queue.sadd('nextURLs', item)
            break


    def start(GetThreadNum, RequestThreadNum, ParseThreadNum):
        for item in range(GetThreadNum):
            ge = ThreadGetURL(next_queue, url_queue)
            ge.start()

        for item in range(RequestThreadNum):
            re = ThreadRequest(url_queue, content_queue)
            re.start()

        for item in range(ParseThreadNum):
            pa = ThreadParse(content_queue)
            pa.start()


    get_next(start_url='https://www.dy2018.com/1/')

    start(GetThreadNum, RequestThreadNum, ParseThreadNum)

```

`config.py`

```python
from redis import StrictRedis, ConnectionPool

URLConfig = {
    'host': '127.0.0.1',
    'password': '123456',
    'db': 0,
    'port': 6379,
}

ContentConfig = {
    'host': '127.0.0.1',
    'password': '123456',
    'db': 0,
    'port': 6379,
}
nextConfig = {
    'host': '127.0.0.1',
    'password': '123456',
    'db': 0,
    'port': 6379,
}

url_pool = ConnectionPool(**URLConfig)
url_queue = StrictRedis(connection_pool=url_pool)

content_pool = ConnectionPool(**ContentConfig)
content_queue = StrictRedis(connection_pool=content_pool)

next_pool = ConnectionPool(**nextConfig)
next_queue = StrictRedis(connection_pool=next_pool)

GetThreadNum = 2

RequestThreadNum = 2

ParseThreadNum = 1
```

