## Hive简介

Hive是基于 Hadoop 的一个【数据仓库工具】，可以将结构化的数据文件映射为一张数据库表，并提供简单的 sql 查询功能，可以将 sql 语句转换为 MapReduce 任务进行运行。



数据仓库是对数据库进行管理和分析，是为企业所有级别的决策制定过程，提供所有类型数据支持的战略集合。它是**单个数据存储**，出于分析性报告和决策支持目的而创建。

**数据仓库是用来做 查询分析的数据库， 基本不用来做插入，修改，删除操作**



数据处理大致可以分成 两大类： **联机事务处理 OLTP**【关系型数据库】（on-line
transaction processing）、 **联机分析处理OLAP**【Hive】（On-Line Analytical
Processing）

​	OLTP是传统的关系型数据库的主要应用，主要是基本的、日常的事务处理。

​	OLAP是数据仓库系统的主要应用，支持复杂的分析操作，侧重决策支持，并且提供直观易懂的查询结果。P是传统的关系型数据库的主要应用。

​	OLTP系统强调数据库内存效率，强调内存各种指标的命令率，强调绑定变量，强调并发操作

​	OLAP系统则强调数据分析，强调SQL执行市场，强调磁盘I/O，强调分区等。

![两种数据处理](..\大数据\img\两种数据处理.png)

Hive:数据仓库。

Hive：解释器，编译器，优化器等。

Hive运行时， **元数据存储在关系型数据库里面**。

编译器将一个Hive SQL转换操作符，操作符是Hive的最小的处理单元每个操作符代表HDFS的一个操作或者一道MapReduce作业。

## Hive架构原理

用户接口主要有三个：CLI命令行，Client 和 WUI

![Hive架构原理](.\img\Hive架构原理.png)

（1）Client是Hive的客户端，用户连接至Hive Server。在启动Client模式的时候，需要指出Hive Server所在节点，并且在该节点启动Hive Server。

 	WUI是通过浏览器访问Hive。

（2）Hive将 元数据存储在（关系）数据库中，如mysql、derby(Hive内嵌的数据库) 。 Hive中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等。

（3）解释器、编译器、优化器完成HQL查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储在HDFS中，并在随后有MapReduce调用执行。

（4）Hive的数据存储在HDFS中，大部分的查询、计算由MapReduce完成（**包含*的查询，比如select * from tbl不会生成MapRedcue任务**）

## Hive环境配置

内嵌模式：元数据保存在内嵌的 derby 中，允许一个会话链接，尝试多个会话链接时会报错

本地模式：本地安装 mysql 替代 derby 存储元数据

远程模式：远程安装 mysql 替代 derby 存储元数据

#### 本地用户模式

【hadoop01 hive，hadoop01 MySQL】

这种安装方式和嵌入式的区别在于，不再使用内嵌的 Derby 作为元数据的存储介质，而是使用其他数据库比如 MySQL 来存储元数据且是一个多用户的模式，运行多个用户 client 连接到一个数据库中。
	这种方式一般作为公司内部同时使用 Hive。这里有一个前提，每一个用户必须要有对 MySQL 的访问权利，即每一个客户端使用者需要知道 MySQL 的用户名和密码才行。
	这种存储方式需要在本地运行一个 mysql 服务器。

###### 1、解压安装包

​	tar -zxvf apache-hive-1.2.1-bin.tar.gz 

###### 2、新建配置文件

​	/home/hive-1.2.1/conf

​	vim hive-site.xml

```
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
	# 存放数据文件(表)
	<property>
		<name>hive.metastore.warehouse.dir</name>
		<value>/user/hive_local/warehouse</value>
	</property>
	# 是否是本地模式
	<property>
		<name>hive.metastore.local</name>
		<value>true</value>
	</property>
	# JDBC的连接（MySQL）
	<property>
		<name>javax.jdo.option.ConnectionURL</name>
		<value>jdbc:mysql://hadoop01/hive_remote?createDatabaseIfNotExist=true</value>
	</property>
	# 驱动jar包
	<property>
		<name>javax.jdo.option.ConnectionDriverName</name>
		<value>com.mysql.jdbc.Driver</value>
	</property>
	# 连接数据库的用户名
	<property>
		<name>javax.jdo.option.ConnectionUserName</name>
		<value>root</value>
	</property>
	# 连接数据库的密码
	<property>
		<name>javax.jdo.option.ConnectionPassword</name>
		<value>123456</value>
	</property>
</configuration>
```

###### 3、配置环境变量

vim ~/.bash_profile

```
export HIVE_HOME=/home/hive-1.2.1
export PATH=$PATH:$HIVE_HOME/bin
```

###### 4、发送配置好的环境变量文件到另外两台服务器上

scp -r ~/.bash_profile hadoop02:/root/

scp -r ~/.bash_profile hadoop03:/root/

###### 5、使环境变量生效

三台机器

source ~/.bash_profile

###### 6、删除jar包

三台机器

rm -rf /home/hadoop-2.6.5/share/hadoop/yarn/lib/jline-0.9.94.jar

###### 7、拷贝jar包

先将本机上的jar包拷贝过去

进入目录：/home/hive-1.2.1/lib

cp -r jline-2.12.jar  /home/hadoop-2.6.5/share/hadoop/yarn/lib/

拷贝到其他两台机器

scp -r jline-2.12.jar  hadoop02:/home/hadoop-2.6.5/share/hadoop/yarn/lib/

scp -r jline-2.12.jar  hadoop03:/home/hadoop-2.6.5/share/hadoop/yarn/lib/

在本机上当前目录下拷贝MySQL jar包

mysql-connector-java-5.1.32-bin.jar

###### 8、安装MySQL

查看是否已经安装过

​	rpm -qa|grep mysql  

卸载当前安装的版本

​	yum -y remove mysql-libs-5.1.71-1.el6.x86_64

安装MySQL

​	yum -y install mysql-server mysql-devel

启动MySQL

​	service mysqld start

​	可以进入MySQL

​		mysql -uroot 

配置超级管理员权限

​	mysqladmin -u root password 123456

​	登录

​		mysql -uroot -p123456

​		如果出错：参照https://blog.csdn.net/Jackie_ZHF/article/details/79368782

​	修改user表，使的外部能够连接

​		use mysql;

​		show tables;

​		select host,user,password  from user;

​		delete from user where password = '';

​		update user set host='%';

​		刷新权限

​			flush privileges;

​		重启mysql服务

​			service mysqld restart

​		mysql -uroot -p123456

###### 9、启动hive

首先启动hadoop集群

​	三台机器：

​		zkServer.sh start

​		zkServer.sh status

​	start-all.sh 

除了主机，其他两台启动resourcemanager

​	yarn-daemon.sh start resourcemanager

启动hive

​	hive



#### 远程一体模式

这种存储方式需要在远端服务器运行一个 mysql 服务器，并且需要在 Hive 服务器启动 meta服务。

在Hadoop03上搭建

###### 1、配置系统文件

/home/hive-1.2.1/conf

​	vim hive-site.xml

```
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
	<property>
		<name>hive.metastore.warehouse.dir</name>
		<value>/user/hive/warehouse2</value>
	</property>
	<property>
		<name>javax.jdo.option.ConnectionURL</name>
		<value>jdbc:mysql://hadoop01:3306/hive?createDatabaseIfNotExist=true</value>
	</property>
	<property>
		<name>javax.jdo.option.ConnectionDriverName</name>
		<value>com.mysql.jdbc.Driver</value>
	</property>
	<property>
		<name>javax.jdo.option.ConnectionUserName</name>
		<value>root</value>
	</property>
	<property>
		<name>javax.jdo.option.ConnectionPassword</name>
		<value>123456</value>
	</property>
	<property>
		<name>hive.metastore.local</name>
		<value>false</value>
	</property>
</configuration>
```

###### 2、在hadoop01 上启动服务器

​	(注意：关闭防火墙：service iptables stop)

​	所有节点：zkServer.sh start

​	start-all.sh 

​	service mysqld start

###### 3、启动hive

hive

#### 远程分开模式

客户端：hadoop02

将 hive-site.xml 配置文件拆为如下两部分
		1 ）、服务端配置文件

```
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
	<property>
		<name>hive.metastore.warehouse.dir</name>
		<value>/user/hive/warehouse</value>
	</property>
	<property>
		<name>javax.jdo.option.ConnectionURL</name>
		<value>jdbc:mysql://hadoop01:3306/hive2?createDatabaseIfNotExist=true</value>
	</property>
	<property>
		<name>javax.jdo.option.ConnectionDriverName</name>
		<value>com.mysql.jdbc.Driver</value>
	</property>
	<property>
		<name>javax.jdo.option.ConnectionUserName</name>
		<value>root</value>
	</property>
	<property>
		<name>javax.jdo.option.ConnectionPassword</name>
		<value>123456</value>
	</property>
</configuration>
```

​	2 ）、客户端配置文件

```
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
	<property>
		<name>hive.metastore.warehouse.dir</name>
		<value>/user/hive/warehouse</value>
	</property>
	<property>
		<name>hive.metastore.local</name>
		<value>false</value>
	</property>
	<property>
		<name>hive.metastore.uris</name>
		<value>thrift://hadoop02:9083</value>
	</property>
</configuration>
```

启动：

​	hive --service metastore

在http://hadoop02:50070/explorer.html#/user/hive/warehouse/ls.db下可以查看



#### 相关命令

show databases;

create database  zs;

use zs;



创建表：

```
create table test01(
id int,
name string,
age int,
likes array<string>,
address map<string,string>
)
row format delimited fields terminated by ','
COLLECTION ITEMS TERMINATED by '-'
map keys terminated by ':';
```

在active状态下的NameNode:

​	http://hadoop01:50070/ 可以看到有数据表的信息

​	http://hadoop01:50070/explorer.html#/user/hive_local/warehouse/zs.db

![Hive启动](./img/Hive%E5%90%AF%E5%8A%A8.png)

quit;

yarn-daemon.sh stop resourcemanager

stop-all.sh

## HQL详解

#### DDL语句

 重点是 hive 的建表语句和分区。

hive数据的定义语言

```
# 创建数据库 查看 show databases；
CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name
[COMMENT database_comment];

# 删除数据库
DROP (DATABASE|SCHEMA) [IF EXISTS] database_name;

# 修改数据库( ( 了解) )
ALTER (DATABASE|SCHEMA) database_name SET DBPROPERTIES
(property_name=property_value, ...);
ALTER (DATABASE|SCHEMA) database_name SET OWNER [USER|ROLE]
user_or_role;

# 使用数据库
USE database_name;
Use default;

```

创键表

```
数据类型：
data_type
: primitive_type 原始数据类型
| array_type 数组
| map_type map
| struct_type
| union_type -- (Note: Available in Hive 0.7.0 and later)
primitive_type
: TINYINT
| SMALLINT
| INT
| BIGINT
| BOOLEAN
| FLOAT
| DOUBLE
| DOUBLE PRECISION
| STRING 基本可以搞定一切
| BINARY
| TIMESTAMP
| DECIMAL
| DECIMAL(precision, scale)
| DATE
| VARCHAR
| CHAR
array_type
: ARRAY < data_type >
map_type
: MAP < primitive_type, data_type >
struct_type
: STRUCT < col_name : data_type [COMMENT col_comment], ...>
union_type
: UNIONTYPE < data_type, data_type, ... >
```

例子

```
create table abc(  # 表明
id int,
name string,
age int,
likes array<string>,
address map<string,string>  # 键值对
)
row format delimited fields terminated by ','  # 字段按‘，’切分
COLLECTION ITEMS TERMINATED by '-'   # 按照‘-’切分
map keys terminated by ':'
lines terminated by '\n';

Select address[‘city’] from person where name=‘zs’;

导入数据（属于 DML 但是为了演示需要在此应用）：
LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE  tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]

从hdfs上导入数据与上述一致
```



外部表( 重要)：外部关键字 EXTERNAL 允许您创建一个表,并提供一个位置,以便 hive 不使用这个表的默认位置。 这方便如果你已经生成的数据。当删除一个外部表, 表中的数据不是从文件系统中删除。外部表
指向任何 HDFS 的存储位置,而不是存储在配置属性指定的文件夹hive.metastore.warehouse.dir.中

```
create EXTERNAL table abc(  # 表明
id int,
name string,
age int,
likes array<string>,
address map<string,string>  # 键值对
)
row format delimited fields terminated by ','  # 字段按‘，’切分
COLLECTION ITEMS TERMINATED by '-'   # 按照‘-’切分
map keys terminated by ':'
lines terminated by '\n';
```



删除表
		DROP TABLE [IF EXISTS] table_name [PURGE];



修改表, 更新，删除数据( 这些很少用)
		重命名表
			ALTER TABLE table_name RENAME TO new_table_name;

​		Eg: alter table meninem rename to jacke;
		更新数据
			UPDATE tablename SET column = value [, column =value ...][WHERE expression]
		删除数据
			DELETE FROM tablename [WHERE expression]

#### DML语句

###### hive数据操作语言

插入数据： 

```
第一种：
    LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLEtablename [PARTITION (partcol1=val1, partcol2=val2 ...)]

第二种： 表的字段一定相同
    创建 person2 表，然后从表 person1.....中导入数据：（注意：字段数目要一致，字段要对应）
    INSERT OVERWRITE TABLE person2 [PARTITION(dt='2008-06-08', country)] SELECT id,name, age From ppt;

第三种：可以多字段表插入少字段表
    from aa
    insert overwrite table bb
    select id,name,age,likes,address
    insert overwrite table cc
    select id,name,age,likes,address ;
    从一个表里查出数据插入到多张表里

    from aa
    insert overwrite table bb
    select id,name,age,likes,address where id=2
    insert overwrite table cc
    select id,name,age,likes,address where id=3

    变态操作 N - N
    FROM aa t1,bb t2
    INSERT OVERWRITE TABLE cc
    SELECT t1.id,t1.name,t1.age,t2.likes,t2.address where t1.id=t2.id
    INSERT OVERWRITE TABLE dd
    SELECT t1.id,t1.name,t1.age,t2.likes,t2.address where t1.id=t2.id
```

###### 查询数据并保存

1、数据存放到本地：

insert overwrite local directory '/home/data/hive_exp_emp' 

ROW FORMAT DELIMITED FIELDS TERMINATED BY ','

select * from t_a ;

在/home/data目录下没有hive_exp_emp 文件，他会自动创建该文件。

2、数据存放HDFS上：

insert overwrite directory '/user/t_a'

select * from t_a ;

3、在 shell 中将数据重定向到文件中：

hive -e 'select * from ls.t_a' > /home/data/log

**必须指定数据库**

###### 数据备份与恢复

备份(备份到集群上):

​	EXPORT TABLE ls.t_a TO ''/home/data/t_a;

恢复：

​	import from '/home/data/t_a';

## Hive SerDe

Hive SerDe - Serializer and Deserializer SerDe 用于做序列化和反序列化。
	构建在数据存储和执行引擎之间，对两者实现解耦。
	Hive 通过 ROW FORMAT DELIMITED 以及 SERDE 进行内容的读写。

Hive正则匹配

```
CREATE TABLE logtbl (
host STRING,
identity STRING,
t_user STRING,
time STRING,
request STRING,
referer STRING,
agent STRING)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'
WITH SERDEPROPERTIES (
"input.regex" = "([^ ]*) ([^ ]*) ([^ ]*) \\[(.*)\\] \"(.*)\" (-|[0-9]*) (-|[0-9]*)"
)
STORED AS TEXTFILE;


LOAD DATA LOCAL INPATH '/home/data/log.txt' OVERWRITE INTO TABLE logtbl;
```

## Beeline 和 Hiveseerver2

hiverserver2 需要一体模式hadoop03服务器上

hiveserver2 

新建本地窗口hadoop03：

beeline

!connect jdbc:hive2://hadoop03:10000 root 123456;

可以很直观明了的看到数据库、表的内部结构 

退出：!quit

## 分区 partition

#### 静态分区

注意：必须在表定义时创建 partition ！

分区分为：单分区和多分区



###### 创建分区

**注意：在创建 删除多分区等操作时一定要注意分区的先后顺序，他们是父子节点的关系。分区字段不要字段相同**

1、单分区建表语句：

```
# 建表
create table day_table
(id int,
content string) partitioned by (dt string)
row format delimited fields terminated by ',';

创建文件存放数据
1,zshang
2,lishi
3,wang2mazi

加载数据
load data local inpath '/home/data/data'into table day_table partition(dt='20180716');

查看表结构
desc day_table;

按分区查找
select * from day_table where dt='20180716';
```

2、双分区建表

```
create table day_hour_table (id int,
content string) partitioned by (dt string, hour string) row
format delimited fields terminated by ',';

load data local inpath '/home/data/data'into table day_hour_table partition(dt='20180714',hour='21');

select * from day_hour_table where dt='20180714' and hour='20'；
```

修改分区名称

ALTER TABLE day_table PARTITION (dt='20180715') RENAME TO PARTITION (dt='2018-07-15');

添加分区

```
分区表已创建，在此基础上添加分区

ALTER TABLE day_table ADD PARTITION (dt='2008-08-08')；

因为新添加的分区上没有数据，所以在select * from day_table;上看不到分区信息

查看分区信息
show partitions day_table;

```

#### 动态分区

```
创建数据文件
aaa,US,CA
aaa,US,CB
bbb,CA,BB
bbb,CA,BC

建立非分区表并加载数据
1. 
CREATE TABLE t1 (name STRING, cty STRING, st STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';

2. LOAD DATA LOCAL INPATH '/home/grid/a.txt' INTO TABLE t1;
3. SELECT * FROM t1;

建立外部分区表并动态加载数据 （注意删除外部表的相关事项）
CREATE EXTERNAL TABLE t2 (name STRING) PARTITIONED BY (country STRING, state
STRING);
```

这时候就需要使用动态分区来实现，使用动态分区需要注意
		设定以下参数：
			• hive.exec.dynamic.partition
				默认值：True
				是否开启动态分区功能，默认 false 关闭。
				使用动态分区时候，该参数必须设置成 true;

```
set hive.exec.dynamic.partition;
hive.exec.dynamic.partition=true
```

​		• hive.exec.dynamic.partition.mode
				默认值：strict
				动态分区的模式，默认 strict，表示必须指定至少一个分区为静态分区，nonstrict 模式表示允许所有的分区字段都可以使用动态分区。
				一般需要设置为 nonstrict

```
set hive.exec.dynamic.partition.mode = nonstrict;
```

​		• hive.exec.max.dynamic.partitions.pernode
				默认值：100
				在每个执行 MR 的节点上，最大可以创建多少个动态分区。
				该参数需要根据实际的数据来设定。
				比如：源数据中包含了一年的数据，即 day 字段有 365 个值，那么该参数就需要设置成大于 365，如果使用默认值 100，则会报错。
			• hive.exec.max.dynamic.partitions
				默认值：1000
				在所有执行 MR 的节点上，最大一共可以创建多少个动态分区。
				同上参数解释。

```
set hive.exec.max.dynamic.partitions;
```

​		• hive.exec.max.created.files
				默认值：100000
				整个 MR Job 中，最大可以创建多少个 HDFS 文件。
				一般默认值足够了，除非你的数据量非常大，需要创建的文件数大于 100000，可根据实际情况加以调整。

```
set hive.exec.max.created.files;
```

​		• hive.error.on.empty.partition

​			默认值：false
				当有空分区生成时，是否抛出异常。
				一般不需要设置。

```
set sethive.error.on.empty.partition
```

```
INSERT INTO TABLE t2 PARTITION (country, state) SELECT name, cty, st FROM t1;

desc t2;
```

#### 自定义函数

自定义函数包括三种 UDF、UDAF、UDTF
		UDF：一进一出
		UDAF：聚集函数，多进一出。如：Count/max/min
		UDTF：一进多出，如 lateral view explore()

## 案例：

wordcount：

```
建表
create table docs(line string);
create table wc(word string, totalword int);

加载数据
load data local inpath '/home/data/wordcount' into table docs;

查询、统计
from (select explode(split(line, ' ')) as word from docs) w
insert into table wc
select word, count(1) as totalword
group by word
order by word;

查询结果
select * from wc;
```

## 分桶

分桶表是对**列值**取哈希值的方式，将不同数据放到不同文件中存储。
	对于 hive 中每一个表、分区都可以进一步进行分桶。
	由列的哈希值除以桶的个数来决定每条数据划分在哪个桶中。
	适用场景：
		数据抽样（ sampling ）、map-join



##### 开启支持分桶

set hive.enforce.bucketing=true;
	默认：false；设置为 true 之后，mr 运行时会根据 bucket 的个数自动分配 reduce task 个数。（用户也可以通过 mapred.reduce.tasks 自己设置 reduce 任务个数，但分桶时不推荐使用）
	注意：一次作业产生的桶（文件数量）和 reduce task 个数一致。

##### 往分桶表中加载数据

insert into table bucket_table select columns from tbl;
	insert overwrite table bucket_table select columns from tbl;

##### 桶表 抽样查询

select * from bucket_table tablesample(bucket 1 out of 4 on columns);
	TABLESAMPLE 语法：
		TABLESAMPLE(BUCKET x OUT OF y)
		x：表示从哪个 bucket 开始抽取数据
		y：必须为该表总 bucket 数的倍数或因子

```
创建表
CREATE TABLE psn31( id INT, name STRING, age INT)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';

创建数据文件
1,tom,11
2,cat,22
3,dog,33
4,hive,44
5,hbase,55
6,mr,66
7,alice,77
8,scala,88

加载文件
load data local inpath '/home/data/test' into table psn31;

创建分桶表
CREATE TABLE psnbucket( id INT, name STRING, age INT)
CLUSTERED BY (age) INTO 4 BUCKETS    按照age 创建4个桶
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';

加载数据
insert into table psnbucket select id, name, age from psn31;

抽样
select id, name, age from psnbucket tablesample(bucket 2 out of 4 on age);
```

抽样的时候x,y可以随机定义，y可以大于分桶数，不过此时x最小为y-分桶数



——？？？？位置



##### Hive Lateral View(UDAF)

Lateral View 用于和 UDTF 函数（explode、split）结合来使用。
	首先通过 UDTF 函数拆分成多行，再将多行结果组合成一个支持别名的虚拟表。
	主要解决在 select 使用 UDTF 做查询过程中，查询只能包含单个 UDTF，不能包含其他字段、以及多个 UDTF 的问题。

```
建表
create table t_a(
id int,
name string,
age int,
likes array<string>,
address map<string,string>
)
row format delimited fields terminated by ','
COLLECTION ITEMS TERMINATED by '-'
map keys terminated by ':';

数据
1,zshang,18,game-girl-book,stu_addr:beijing-work_addr:shanghai
2,lishi,16,shop-boy-book,stu_addr:hunan-work_addr:shanghai
3,wang2mazi,20,fangniu-eat,stu_addr:shanghai-work_addr:tianjing

加载数据
LOAD DATA LOCAL INPATH '/home/data/t_a' OVERWRITE INTO TABLE t_a;


select count(distinct(myCol1)), count(distinct(myCol2)) from t_a
LATERAL VIEW explode(likes) myTable1 AS myCol1   		# explode UDTF函数
LATERAL VIEW explode(address) myTable2 AS myCol2, myCol3;

结果
7   2
7个爱好，2种地址

select count(distinct(myCol1)), count(distinct(myCol2)),count(distinct(myCol3))from t_a
LATERAL VIEW explode(likes) myTable1 AS myCol1    # explode UDTF函数
LATERAL VIEW explode(address) myTable2 AS myCol2, myCol3;
结果
7	2	4
7个爱好，2种地址，4个地址
```

##### 运行方式

1、client控制台

```
dfs -ls /;
dfs -ls /user;
```

linux

```
!cat /root/zookeeper.out；
!mkdir -p /root/zs；
```

2、脚本运行方式

hive 脚本

```
hive -e ''

vi test
	select * from t_a;
hive -f test

hive控制台下
source /root/zs/test;
```



3、JDBC：hiveserver2

4、web GUI接口

```
Hive Web GUI 接口
web 界面安装：
    1、下载源码包 apache-hive-*-src.tar.gz
    2、将 hwi war 包放在$HIVE_HOME/lib/
        制作方法：将 hwi/web/*里面所有的文件打成 war 包
        cd apache-hive-1.2.1-src/hwi/web
        jar -cvf hive-hwi.war *
    3、复制 tools.jar(在 jdk 的 lib 目录下)到$HIVE_HOME/lib 下
    4、修改 hive-site.xml
        <property>
        <name>hive.hwi.listen.host</name>
        <value>0.0.0.0</value>
        </property>
        <property>
        <name>hive.hwi.listen.port</name>
        <value>9999</value>
        </property>
        <property>
        <name>hive.hwi.war.file</name>
        <value>lib/hive-hwi.war</value>
        </property>
    5、启动 hwi 服务(端口号 9999)
        hive --service hwi
    6、浏览器通过以下链接来访问
```

## 	权限管理

三种授权模型：

​	1 、Storage Based Authorization in the Metastore Server 基于存储的授权

​		可以对 Metastore 中的元数据进行保护，但是没有提供更加细粒度的访问控制（例如：列级别、行级别）。

​	2 、SQL Standards Based Authorization in HiveServer2 基于 SQL 标准的 Hive 授权

​		完全兼容 SQL 的授权模型，推荐使用该模式。

​	3 、Default Hive Authorization (Legacy Mode) 	hive 默认授权

​		设计目的仅仅只是为了防止用户产生误操作，而不是防止恶意用户访问未经授权的数据。
			基于 SQL 标准的完全兼容 SQL 的授权模型，除支持对于用户的授权认证，还支持角色 role 的授权认证。
			role 可理解为是一组权限的集合，通过 role 为用户授权一个用户可以具有一个或多个角色。

​		默认包含两种角色：public、admin



​	限制：
			1、启用当前认证方式之后，dfs, add, delete, compile, and reset 等命令被禁用。
			2、通过 set 命令设置 hive configuration 的方式被限制某些用户使用。（可通过修改配置文件 hive-site.xml 中hive.security.authorization.sqlstd.confwhitelist 进行配置）
			3、添加、删除函数以及宏（批量规模）的操作，仅为具有 admin 的用户开放。
			4、用户自定义函数（开放支持永久的自定义函数），可通过具有 admin 角色的用户创建，其他用户都可以使用。
			5、Transform 功能被禁用

##### 环境配置

vim /home/hive-1.2.1/conf/hive-site.xml

```
<property>
    <name>hive.security.authorization.enabled</name>
    <value>true</value>
</property>
<property>
    <name>hive.server2.enable.doAs</name>
    <value>false</value>
</property>
<property>
    <name>hive.users.in.admin.role</name>
    <value>root</value>
</property>
<property>
    <name>hive.security.authorization.manager</name>   								<value>org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory</value>
</property>
<property>
	<name>hive.security.authenticator.manager</name>
	<value>org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator</value>
</property>
```

##### 相关命令

服务端启动 hiveserver2 ；客户端通过 beeline 进行连接

!connect jdbc:hive2://hadoop03:10000 root 123456;

```
初始命令会错，需要设置角色
0: jdbc:hive2://hadoop03:10000> create role ls
0: jdbc:hive2://hadoop03:10000> show roles
0: jdbc:hive2://hadoop03:10000> ;
Error: Error while compiling statement: FAILED: ParseException line 2:0 missing EOF at 'show' near 'ls' (state=42000,code=40000)
0: jdbc:hive2://hadoop03:10000> create role ls;
Error: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Current user : root is not allowed to add roles. User has to belong to ADMIN role and have it as current role, for this action. (state=08S01,code=1)

0: jdbc:hive2://hadoop03:10000> SET ROLE ADMIN;
No rows affected (0.045 seconds)
0: jdbc:hive2://hadoop03:10000> create role ls;
No rows affected (0.052 seconds)
0: jdbc:hive2://hadoop03:10000> show roles;
+---------+--+
|  role   |
+---------+--+
| admin   |
| ls      |
| public  |
+---------+--+
3 rows selected (0.04 seconds)
0: jdbc:hive2://hadoop03:10000> SHOW CURRENT ROLES;
+--------+--+
|  role  |
+--------+--+
| admin  |
+--------+--+
1 row selected (0.028 seconds)

```

```
show databases;			查看数据库
CREATE ROLE role_name; 	 创建角色
DROP ROLE role_name;		删除角色
SET ROLE (role_name|ALL|NONE);	 设置角色
SHOW CURRENT ROLES; 	查看当前具有的角色
SHOW ROLES;		 查看所有存在的角色
```

![Hive权限管理-角色的授予、移除、查看](.\img\Hive权限管理-角色的授予、移除、查看.png)



![Hive权限管理-权限的授予、移除、查看](.\img\Hive权限管理-权限的授予、移除、查看.png)

## Hive的优化

（启动之前需要把之前权限的配置删除）

核心思想：把Hive SQL 当做Mapreduce程序去优化 

以下SQL不会转为Mapreduce来执行 （使用函数会转化MapReduce）

```
select * from t_a;

select count(*) from t_a; 需要转化成MapReduce
```

```
--select仅查询本表字段
--where仅对本表字段做条件过滤
```

Explain 显示执行计划

EXPLAIN [EXTENDED] query 

```
Explain select count(id) from t_a where id > 2;
```

Hive运行方式：     本地模式                        100MB 

​				集群模式

===================================================

##### **本地模式**

set hive.exec.mode.local.auto=true;

**注意**

hive.exec.mode.local.auto.inputbytes.max默认值为128M

表示加载文件的最大值，若大于该配置仍会以集群方式来运行！

===================================================

##### **严格模式** 

通过设置以下参数开启严格模式[防止误操作]： 

set hive.mapred.mode=strict; 

（默认为：nonstrict非严格模式）

###### 查询限制 

1、对分区表查询时，必须添加where对于分区字段的条件过滤；

​	不是分区表不影响

```
分区表
hive> select * from day_table;
FAILED: SemanticException [Error 10041]: No partition predicate found for Alias "day_table" Table "day_table"
hive> desc day_table;
OK
id                  	int                 	                    
content             	string              	                    
dt                  	string              	                    
	 	 
# Partition Information	 	 
# col_name            	data_type           	comment             
	 	 
dt                  	string              	                    
Time taken: 0.092 seconds, Fetched: 8 row(s)

非分区表
hive> select * from t_a;
OK
1	zshang	18	["game","girl","book"]	{"stu_addr":"beijing","work_addr":"shanghai"}
2	lishi	16	["shop","boy","book"]	{"stu_addr":"hunan","work_addr":"shanghai"}
3	wang2mazi	20	["fangniu","eat"]	{"stu_addr":"shanghai","work_addr":"tianjing"}
Time taken: 0.082 seconds, Fetched: 3 row(s)
```

2、order by语句必须包含limit输出限制；

```
hive> select * from t_a order by name;
FAILED: SemanticException 1:27 In strict mode, if ORDER BY is specified, LIMIT must also be specified. Error encountered near token 'name'

hive> select * from t_a order by name limit 1;
Automatically selecting local only mode for query
Query ID = root_20180718045437_a4c5abc4-c8a3-4e3f-9561-a99e3adf6144
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2018-07-18 04:54:39,029 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local741013642_0002
MapReduce Jobs Launched: 
Stage-Stage-1:  HDFS Read: 2236 HDFS Write: 63 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
2	lishi	16	["shop","boy","book"]	{"stu_addr":"hunan","work_addr":"shanghai"}
Time taken: 1.412 seconds, Fetched: 1 row(s)
```

3、限制执行笛卡尔积的查询 

=========================================================

###### Hive排序 

1、Order By - 对于查询结果做全排序，只允许有一个reduce处理 

**海量数据需要设置多个reduce来处理**

（当数据量较大时，应慎用。严格模式下，必须结合limit来使用） 

2、Sort By - 对于单个reduce的数据进行排序 

3、Distribute By - 分区排序，经常和Sort By结合使用 

4、Cluster By - 相当于 Sort By + Distribute By 

可以做到全排序

（Cluster By不能通过asc、desc的方式指定排序规则； 可通过 distribute by column sort by column asc|desc 的方式） 

=========================================================

Hive Join   

**Join计算时，将小表（驱动表）放在join的左边** 

Map Join：在Map端完成Join 

两种实现方式： 

1、SQL方式，在SQL语句中添加MapJoin标记（mapjoin hint） 

语法： 

SELECT  **/*+ MAPJOIN(smallTable) */**  smallTable.key,  bigTable.value 

 FROM  smallTable  JOIN  bigTable  ON  smallTable.key  =  bigTable.key; 

2、开启自动的MapJoin 

应该把小表放前面，大表放后面，因为hive默认开启Mapjoin   hive.auto.convert.join=true,所以会默认的把左边的看成小表加载到内存中。

通过修改以下配置启用自动的mapjoin： 

set hive.auto.convert.join = true; （

该参数为true时，Hive自动对左边的表统计量，如果是小表就加入内存，即对小表使用Map join） 

其他相关配置参数： hive.mapjoin.smalltable.filesize;   

（大表小表判断的阈值，如果表的大小小于该值则会被加载到内存中运行） 

```
默认25M
hive> set hive.mapjoin.smalltable.filesize;
hive.mapjoin.smalltable.filesize=25000000
```



hive.ignore.mapjoin.hint;

（默认值：true；是否忽略mapjoin hint 即mapjoin标记）

```
hive> set hive.ignore.mapjoin.hint;
hive.ignore.mapjoin.hint=true
默认进行mapjoin
```



hive.auto.convert.join.noconditionaltask; 

（默认值：true；将普通的join转化为普通的mapjoin时，是否将多个mapjoin转化为一个mapjoin）



hive.auto.convert.join.noconditionaltask.size; 

（将多个mapjoin转化为一个mapjoin时，其表的最大值 ）

```
默认10M
hive> set hive.auto.convert.join.noconditionaltask.size; 
hive.auto.convert.join.noconditionaltask.size=10000000
```

=========================================================

###### Map-Side聚合   

如count()等聚合函数 

通过设置以下参数开启在Map端的聚合：

set hive.map.aggr=true;  

相关配置参数： 

```
hive.groupby.mapaggr.checkinterval;  
map端group by执行聚合时处理的多少行数据（默认：100000）  

set hive.map.aggr.hash.min.reduction;  
默认0.5
进行聚合的最小比例（预先对100000条数据做聚合，若聚合之后的数据量/100000的值大于该配置0.5，则不会聚合）  

hive.map.aggr.hash.percentmemory;
默认0.5=======50M 因为map是100M
map端聚合使用的内存的最大值  

hive.map.aggr.hash.force.flush.memory.threshold;  
map端做聚合操作是hash表的最大可用内容，大于该值则会触发flush 


hive.groupby.skewindata;
是否对GroupBy产生的数据倾斜做优化，默认为false 
```

=========================================================

###### 控制Hive中Map以及Reduce的数量 

```
Map数量相关的参数 

    mapred.max.split.size 
        hive> set  mapred.max.split.size ;
        mapred.max.split.size=256000000
        一个split的最大值，即每个map处理文件的最大值 

    mapred.min.split.size.per.node 
        hive> set mapred.min.split.size.per.node ;
        mapred.min.split.size.per.node=1
        一个节点上split的最小值 

    mapred.min.split.size.per.rack 
        hive> set mapred.min.split.size.per.rack ;
        mapred.min.split.size.per.rack=1

        一个机架上split的最小值   


Reduce数量相关的参数 

    mapred.reduce.tasks 
        强制指定reduce任务的数量 

    hive.exec.reducers.bytes.per.reducer 
        hive> set hive.exec.reducers.bytes.per.reducer;
        hive.exec.reducers.bytes.per.reducer=256000000
        每个reduce任务处理的数据量 
        因为一个切片最大256，一个reduce可以处理多个切片，所以最大256

    hive.exec.reducers.max 
        hive> set hive.exec.reducers.max ;
        hive.exec.reducers.max=1009
        最少1个
        每个任务最大的reduce数 
        Map数量>Reduce数量
```

=========================================================

##### Hive - JVM重用

适用场景： 

1、小文件个数过多

2、task个数过多

通过 set mapred.job.reuse.jvm.num.tasks=n; 来设置

默认为1

（n为task插槽个数）一个Map一个task

缺点：设置开启之后，task插槽会一直占用资源，不论是否有task运行，

直到所有的task即整个job全部执行完成时，才会释放所有的task插槽资源！